{
  "4": {
    "inputs": {
      "images": [
        "5",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "5": {
    "inputs": {
      "transparency": true,
      "model": "u2net",
      "post_processing": false,
      "only_mask": false,
      "alpha_matting": false,
      "alpha_matting_foreground_threshold": 240,
      "alpha_matting_background_threshold": 10,
      "alpha_matting_erode_size": 33,
      "background_color": "none",
      "images": [
        "131",
        0
      ]
    },
    "class_type": "Image Rembg (Remove Background)",
    "_meta": {
      "title": "Image Rembg (Remove Background)"
    }
  },
  "10": {
    "inputs": {
      "images": [
        "103",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "13": {
    "inputs": {
      "text": "("
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "14": {
    "inputs": {
      "text": [
        "46",
        0
      ]
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "15": {
    "inputs": {
      "text": ")"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "16": {
    "inputs": {
      "delimiter": " ",
      "clean_whitespace": "true",
      "text_a": [
        "19",
        0
      ],
      "text_b": [
        "17",
        0
      ],
      "text_c": [
        "18",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "17": {
    "inputs": {
      "value": [
        "51",
        0
      ]
    },
    "class_type": "PrimitiveString",
    "_meta": {
      "title": "Î¨∏ÏûêÏó¥"
    }
  },
  "18": {
    "inputs": {
      "text": ")"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "19": {
    "inputs": {
      "text": "(wearing "
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "22": {
    "inputs": {
      "lora_name": "ip-adapter-faceid-plusv2_sd15_lora.safetensors",
      "strength_model": 1,
      "model": [
        "106",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoRA Î°úÎìú (Î™®Îç∏ Ï†ÑÏö©)"
    }
  },
  "23": {
    "inputs": {
      "ckpt_name": "realisticVisionV51_v51VAE.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú"
    }
  },
  "25": {
    "inputs": {
      "ipadapter_file": "ip-adapter-faceid-plusv2_sd15.bin"
    },
    "class_type": "IPAdapterModelLoader",
    "_meta": {
      "title": "IPAdapter Model Loader"
    }
  },
  "29": {
    "inputs": {
      "model_name": "sam_vit_h (2.56GB)"
    },
    "class_type": "SAMModelLoader (segment anything)",
    "_meta": {
      "title": "SAMModelLoader (segment anything)"
    }
  },
  "30": {
    "inputs": {
      "model_name": "GroundingDINO_SwinT_OGC (694MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "31": {
    "inputs": {
      "images": [
        "76",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "32": {
    "inputs": {
      "model_name": "GroundingDINO_SwinT_OGC (694MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "33": {
    "inputs": {
      "images": [
        "77",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "34": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "CLIP_VISION Î°úÎìú"
    }
  },
  "35": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "CLIP_VISION Î°úÎìú"
    }
  },
  "36": {
    "inputs": {
      "text": "(waring "
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "39": {
    "inputs": {
      "text_input": "",
      "task": "prompt_gen_mixed_caption_plus",
      "fill_mask": false,
      "keep_model_loaded": false,
      "max_new_tokens": 4096,
      "num_beams": 6,
      "do_sample": false,
      "output_mask_select": "",
      "seed": "{seed4}",
      "image": [
        "139",
        0
      ],
      "florence2_model": [
        "41",
        0
      ]
    },
    "class_type": "Florence2Run",
    "_meta": {
      "title": "Florence2Run"
    }
  },
  "40": {
    "inputs": {
      "delimiter": " ",
      "clean_whitespace": "true",
      "text_a": [
        "36",
        0
      ],
      "text_b": [
        "39",
        2
      ],
      "text_c": [
        "44",
        2
      ],
      "text_d": [
        "43",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "41": {
    "inputs": {
      "model": "MiaoshouAI/Florence-2-large-PromptGen-v2.0",
      "precision": "fp16",
      "attention": "sdpa",
      "convert_to_safetensors": false
    },
    "class_type": "DownloadAndLoadFlorence2Model",
    "_meta": {
      "title": "DownloadAndLoadFlorence2Model"
    }
  },
  "42": {
    "inputs": {
      "weight": 0.7000000000000002,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "75",
        0
      ],
      "ipadapter": [
        "75",
        1
      ],
      "image": [
        "5",
        0
      ],
      "attn_mask": [
        "77",
        1
      ],
      "clip_vision": [
        "35",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "43": {
    "inputs": {
      "text": ")"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "44": {
    "inputs": {
      "text_input": "",
      "task": "prompt_gen_mixed_caption_plus",
      "fill_mask": false,
      "keep_model_loaded": false,
      "max_new_tokens": 4096,
      "num_beams": 6,
      "do_sample": false,
      "output_mask_select": "",
      "seed": "{seed5}",
      "image": [
        "131",
        0
      ],
      "florence2_model": [
        "41",
        0
      ]
    },
    "class_type": "Florence2Run",
    "_meta": {
      "title": "Florence2Run"
    }
  },
  "46": {
    "inputs": {
      "text": "{human_info}"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "47": {
    "inputs": {
      "text": "{prompt_text}"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "48": {
    "inputs": {
      "text": "{clothes_type2}"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "49": {
    "inputs": {
      "text": "{clothes_type1}"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "51": {
    "inputs": {
      "delimiter": " and ",
      "clean_whitespace": "true",
      "text_a": [
        "49",
        0
      ],
      "text_b": [
        "48",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "52": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "CLIP_VISION Î°úÎìú"
    }
  },
  "53": {
    "inputs": {
      "provider": "CPU",
      "model_name": "buffalo_l"
    },
    "class_type": "IPAdapterInsightFaceLoader",
    "_meta": {
      "title": "IPAdapter InsightFace Loader"
    }
  },
  "57": {
    "inputs": {
      "text_input": "",
      "task": "prompt_gen_mixed_caption_plus",
      "fill_mask": false,
      "keep_model_loaded": false,
      "max_new_tokens": 1024,
      "num_beams": 6,
      "do_sample": false,
      "output_mask_select": "",
      "seed": "{seed1}",
      "image": [
        "129",
        0
      ],
      "florence2_model": [
        "41",
        0
      ]
    },
    "class_type": "Florence2Run",
    "_meta": {
      "title": "Florence2Run"
    }
  },
  "58": {
    "inputs": {
      "text": "("
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "59": {
    "inputs": {
      "text": "single person, solo, centered, symmetrical composition, fashion model, dynamic pose, studio lighting, wide shot, high quality, ultra-detailed, 8k, DSLR, photo shoot, realistic, sharp focus, natural skin texture\n"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "60": {
    "inputs": {
      "delimiter": ",",
      "clean_whitespace": "true",
      "text_a": [
        "16",
        0
      ],
      "text_b": [
        "40",
        0
      ],
      "text_c": [
        "62",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "61": {
    "inputs": {
      "text": "(full body:1.3), (standing, front-facing:1.3), (white background:1.3)"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "62": {
    "inputs": {
      "delimiter": " ",
      "clean_whitespace": "true",
      "text_a": [
        "13",
        0
      ],
      "text_b": [
        "14",
        0
      ],
      "text_c": [
        "15",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "63": {
    "inputs": {
      "delimiter": ", ",
      "clean_whitespace": "true",
      "text_a": [
        "61",
        0
      ],
      "text_b": [
        "60",
        0
      ],
      "text_c": [
        "59",
        0
      ],
      "text_d": [
        "66",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "64": {
    "inputs": {
      "delimiter": ", ",
      "clean_whitespace": "true",
      "text_a": [
        "47",
        0
      ],
      "text_b": [
        "63",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "65": {
    "inputs": {
      "text": ")"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "66": {
    "inputs": {
      "delimiter": "",
      "clean_whitespace": "true",
      "text_a": [
        "58",
        0
      ],
      "text_b": [
        "57",
        2
      ],
      "text_c": [
        "65",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "69": {
    "inputs": {
      "model_name": "sam_vit_h (2.56GB)"
    },
    "class_type": "SAMModelLoader (segment anything)",
    "_meta": {
      "title": "SAMModelLoader (segment anything)"
    }
  },
  "73": {
    "inputs": {
      "weight": 0.7000000000000002,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "108",
        0
      ],
      "ipadapter": [
        "108",
        1
      ],
      "image": [
        "103",
        0
      ],
      "attn_mask": [
        "76",
        1
      ],
      "clip_vision": [
        "34",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "75": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "73",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "76": {
    "inputs": {
      "prompt": [
        "49",
        0
      ],
      "threshold": 0.3,
      "sam_model": [
        "29",
        0
      ],
      "grounding_dino_model": [
        "30",
        0
      ],
      "image": [
        "119",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "77": {
    "inputs": {
      "prompt": [
        "48",
        0
      ],
      "threshold": 0.3,
      "sam_model": [
        "69",
        0
      ],
      "grounding_dino_model": [
        "32",
        0
      ],
      "image": [
        "119",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "80": {
    "inputs": {
      "x": 0,
      "y": 0,
      "operation": "add",
      "destination": [
        "76",
        1
      ],
      "source": [
        "77",
        1
      ]
    },
    "class_type": "MaskComposite",
    "_meta": {
      "title": "ÎßàÏä§ÌÅ¨ Ìï©ÏÑ±"
    }
  },
  "81": {
    "inputs": {
      "pixels": [
        "119",
        0
      ],
      "vae": [
        "23",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Ïù∏ÏΩîÎìú"
    }
  },
  "84": {
    "inputs": {
      "mask": [
        "80",
        0
      ]
    },
    "class_type": "MaskPreview+",
    "_meta": {
      "title": "üîß Mask Preview"
    }
  },
  "85": {
    "inputs": {
      "samples": [
        "81",
        0
      ],
      "mask": [
        "80",
        0
      ]
    },
    "class_type": "SetLatentNoiseMask",
    "_meta": {
      "title": "Ïû†Ïû¨ Îç∞Ïù¥ÌÑ∞Ïóê ÎÖ∏Ïù¥Ï¶à ÎßàÏä§ÌÅ¨ ÏÑ§Ï†ï"
    }
  },
  "86": {
    "inputs": {
      "seed": "{seed2}",
      "steps": 20,
      "cfg": 10,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "42",
        0
      ],
      "positive": [
        "127",
        0
      ],
      "negative": [
        "127",
        1
      ],
      "latent_image": [
        "85",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "87": {
    "inputs": {
      "images": [
        "89",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "89": {
    "inputs": {
      "samples": [
        "86",
        0
      ],
      "vae": [
        "23",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE ÎîîÏΩîÎìú"
    }
  },
  "96": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "97": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "98": {
    "inputs": {
      "images": [
        "99",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "99": {
    "inputs": {
      "guide_size": 384,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 187270156334601,
      "steps": 10,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": true,
      "noise_mask_feather": 0,
      "tiled_encode": false,
      "tiled_decode": false,
      "image": [
        "89",
        0
      ],
      "model": [
        "22",
        0
      ],
      "clip": [
        "23",
        1
      ],
      "vae": [
        "23",
        2
      ],
      "positive": [
        "127",
        0
      ],
      "negative": [
        "127",
        1
      ],
      "bbox_detector": [
        "97",
        0
      ],
      "sam_model_opt": [
        "96",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "ÏñºÍµ¥ ÎîîÌÖåÏùºÎü¨"
    }
  },
  "101": {
    "inputs": {
      "image": "{image2}"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ Î°úÎìú"
    }
  },
  "102": {
    "inputs": {
      "image": "{image1}"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ Î°úÎìú"
    }
  },
  "103": {
    "inputs": {
      "transparency": true,
      "model": "u2net",
      "post_processing": false,
      "only_mask": false,
      "alpha_matting": false,
      "alpha_matting_foreground_threshold": 240,
      "alpha_matting_background_threshold": 10,
      "alpha_matting_erode_size": 0,
      "background_color": "none",
      "images": [
        "139",
        0
      ]
    },
    "class_type": "Image Rembg (Remove Background)",
    "_meta": {
      "title": "Image Rembg (Remove Background)"
    }
  },
  "106": {
    "inputs": {
      "weight": 1,
      "weight_faceidv2": 1,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "23",
        0
      ],
      "ipadapter": [
        "25",
        0
      ],
      "image": [
        "102",
        0
      ],
      "clip_vision": [
        "52",
        0
      ],
      "insightface": [
        "53",
        0
      ]
    },
    "class_type": "IPAdapterFaceID",
    "_meta": {
      "title": "IPAdapter FaceID"
    }
  },
  "108": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "23",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "110": {
    "inputs": {
      "images": [
        "129",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "112": {
    "inputs": {
      "images": [
        "115",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "113": {
    "inputs": {
      "image": [
        "114",
        0
      ]
    },
    "class_type": "ImageSize",
    "_meta": {
      "title": "Image Size"
    }
  },
  "114": {
    "inputs": {
      "max_width": 512,
      "max_height": 760,
      "min_width": 0,
      "min_height": 0,
      "method": "lanczos",
      "images": [
        "116",
        0
      ]
    },
    "class_type": "ImageTransformResizeClip",
    "_meta": {
      "title": "ImageTransformResizeClip"
    }
  },
  "115": {
    "inputs": {
      "detect_hand": "enable",
      "detect_body": "enable",
      "detect_face": "enable",
      "resolution": 512,
      "scale_stick_for_xinsr_cn": "disable",
      "image": [
        "116",
        0
      ]
    },
    "class_type": "OpenposePreprocessor",
    "_meta": {
      "title": "OpenPose Pose"
    }
  },
  "116": {
    "inputs": {
      "image": "{image3}"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ Î°úÎìú"
    }
  },
  "118": {
    "inputs": {
      "images": [
        "119",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "119": {
    "inputs": {
      "samples": [
        "120",
        0
      ],
      "vae": [
        "23",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE ÎîîÏΩîÎìú"
    }
  },
  "120": {
    "inputs": {
      "seed": "{seed3}",
      "steps": 30,
      "cfg": 6,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "23",
        0
      ],
      "positive": [
        "127",
        0
      ],
      "negative": [
        "127",
        1
      ],
      "latent_image": [
        "128",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "121": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_openpose.pth"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Ïª®Ìä∏Î°§ÎÑ∑ Î™®Îç∏ Î°úÎìú"
    }
  },
  "122": {
    "inputs": {
      "text": [
        "64",
        0
      ],
      "clip": [
        "23",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP ÌÖçÏä§Ìä∏ Ïù∏ÏΩîÎî© (ÌîÑÎ°¨ÌîÑÌä∏)"
    }
  },
  "123": {
    "inputs": {
      "text": "blurry, low quality, low resolution, low contrast, deformed, disfigured, extra limbs, fused fingers, cropped, out of frame, bad anatomy, bad proportions, overexposed, underexposed, noisy, cartoon, anime, painting, illustration, text, watermark, logo, nsfw, grainy, unnatural skin, jpeg artifacts,\nhat, helmet, veil, accessory, multiple people, group, crowd, extra faces, duplicate, clones, twin, another person, second person\n",
      "clip": [
        "23",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP ÌÖçÏä§Ìä∏ Ïù∏ÏΩîÎî© (ÌîÑÎ°¨ÌîÑÌä∏)"
    }
  },
  "127": {
    "inputs": {
      "strength": 1,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "122",
        0
      ],
      "negative": [
        "123",
        0
      ],
      "control_net": [
        "121",
        0
      ],
      "image": [
        "115",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Ïª®Ìä∏Î°§ÎÑ∑ Ï†ÅÏö©"
    }
  },
  "128": {
    "inputs": {
      "width": [
        "113",
        0
      ],
      "height": [
        "113",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Îπà Ïû†Ïû¨ Ïù¥ÎØ∏ÏßÄ"
    }
  },
  "129": {
    "inputs": {
      "transparency": false,
      "model": "u2net",
      "post_processing": false,
      "only_mask": false,
      "alpha_matting": false,
      "alpha_matting_foreground_threshold": 240,
      "alpha_matting_background_threshold": 10,
      "alpha_matting_erode_size": 0,
      "background_color": "none",
      "images": [
        "106",
        1
      ]
    },
    "class_type": "Image Rembg (Remove Background)",
    "_meta": {
      "title": "Image Rembg (Remove Background)"
    }
  },
  "130": {
    "inputs": {
      "model_name": "sam_vit_h (2.56GB)"
    },
    "class_type": "SAMModelLoader (segment anything)",
    "_meta": {
      "title": "SAMModelLoader (segment anything)"
    }
  },
  "131": {
    "inputs": {
      "prompt": [
        "48",
        0
      ],
      "threshold": 0.3,
      "sam_model": [
        "130",
        0
      ],
      "grounding_dino_model": [
        "134",
        0
      ],
      "image": [
        "101",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "132": {
    "inputs": {
      "images": [
        "131",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "134": {
    "inputs": {
      "model_name": "GroundingDINO_SwinT_OGC (694MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "138": {
    "inputs": {
      "model_name": "sam_vit_h (2.56GB)"
    },
    "class_type": "SAMModelLoader (segment anything)",
    "_meta": {
      "title": "SAMModelLoader (segment anything)"
    }
  },
  "139": {
    "inputs": {
      "prompt": [
        "49",
        0
      ],
      "threshold": 0.3,
      "sam_model": [
        "138",
        0
      ],
      "grounding_dino_model": [
        "142",
        0
      ],
      "image": [
        "101",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "140": {
    "inputs": {
      "images": [
        "139",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞"
    }
  },
  "142": {
    "inputs": {
      "model_name": "GroundingDINO_SwinT_OGC (694MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "144": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "99",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•"
    }
  }
}